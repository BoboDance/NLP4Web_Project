\relax 
\@writefile{toc}{\contentsline {section}{\numberline {I}Introduction and Research Problem}{1}}
\@writefile{toc}{\contentsline {section}{\numberline {II}Twitter Crawler}{1}}
\@writefile{toc}{\contentsline {section}{\numberline {III}Data Provisioning}{2}}
\newlabel{sec:provisioning}{{III}{2}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces JSON representation of a tweet written by Barack Obama that was crawled from Twitter}}{2}}
\@writefile{toc}{\contentsline {section}{\numberline {IV}Pipeline}{2}}
\newlabel{sec:pipeline}{{IV}{2}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Visualization of the pipeline built for the authorship identification. After providing the data by the crawler, the data is preprocessed. The feature extraction phase takes care of creating useful features for classification for the training of the model. The last step is the evaluation of the model.}}{3}}
\@writefile{toc}{\contentsline {section}{\numberline {V}Feature Engineering}{3}}
\newlabel{sec:features}{{V}{3}}
\@writefile{lot}{\contentsline {table}{\numberline {I}{\ignorespaces List of features which were considered for training the classifiers.}}{3}}
\newlabel{tab:features}{{I}{3}}
\@writefile{toc}{\contentsline {section}{\numberline {VI}Evaluation}{4}}
\newlabel{sec:training-eval}{{VI}{4}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Evaluation results for different feature setups. Results for \textbf  {Naive Bayes} classifier}}{4}}
\newlabel{fig:results-nb}{{3}{4}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Evaluation results for different feature setups. Results for \textbf  {Random Forest} classifier}}{4}}
\newlabel{fig:results-rf}{{4}{4}}
\@writefile{lot}{\contentsline {table}{\numberline {II}{\ignorespaces Evaluation results for different feature setups ordered by increasing accuracy. Classifier = \textbf  {Naive Bayes}}}{5}}
\newlabel{tab:results-nb}{{II}{5}}
\@writefile{toc}{\contentsline {section}{\numberline {VII}Deep Learning}{5}}
\@writefile{lot}{\contentsline {table}{\numberline {III}{\ignorespaces Evaluation results for different feature setups ordered by increasing accuracy. Classifier = \textbf  {Random Forests}}}{5}}
\newlabel{tab:results-rf}{{III}{5}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Evaluation results for different feature setups. Results for \textbf  {Logistic Regression} classifier}}{5}}
\newlabel{fig:results-logistic}{{5}{5}}
\bibcite{Zheng06}{1}
\bibcite{Hout07}{2}
\bibcite{Fissette10}{3}
\bibcite{Green13}{4}
\@writefile{lot}{\contentsline {table}{\numberline {IV}{\ignorespaces Evaluation results for different feature setups ordered by increasing accuracy. Classifier = \textbf  {Logistic Regression}}}{6}}
\newlabel{tab:results-logistic}{{IV}{6}}
\@writefile{toc}{\contentsline {section}{\numberline {VIII}Component Diagram}{6}}
\@writefile{toc}{\contentsline {section}{\numberline {IX}Conclusion}{6}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces A simple neural network with two hidden layers.}}{6}}
\@writefile{toc}{\contentsline {section}{References}{6}}
\bibcite{Heylighen02}{5}
\bibcite{Aihara15}{6}
\bibcite{Tweedie98}{7}
\bibcite{Bhatia}{8}
